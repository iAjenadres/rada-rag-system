# Copyright (c) 2025 Andr√©s Garc√≠a
# Licensed under the MIT License

"""
ConfluenteConnector para RADA - Sistema RAG
Version FILTRADA - Extracci√≥n directa sin navegaci√≥n recursiva
"""

__author__ = "Andr√©s Garc√≠a"
__version__ = "8.0.DIRECT-EXTRACTION"

import requests
import base64
import json
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import time
import re
from bs4 import BeautifulSoup
import html2text
import os
from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ConfluenteConnectorFiltered:
    """
    Conector especializado para Confluence - EXTRACCI√ìN DIRECTA
    Extrae documentos espec√≠ficos por sus IDs exactos
    """

    def __init__(self,
                 confluence_url: str = None,
                 username: str = None,
                 api_token: str = None,
                 space_key: str = None):

        self.confluence_url = confluence_url or os.getenv(
            'CONFLUENCE_URL', 'https://company.atlassian.net')
        self.username = username or os.getenv('CONFLUENCE_USERNAME')
        self.api_token = api_token or os.getenv('CONFLUENCE_API_TOKEN')
        self.space_key = space_key or os.getenv('CONFLUENCE_SPACE_KEY', 'DOCS')

        if self.confluence_url:
            self.confluence_url = self.confluence_url.rstrip('/')

        self.wiki_base_url = f"{self.confluence_url}/wiki"
        self.rest_api_url = f"{self.wiki_base_url}/rest/api"

        self.session = self._setup_authentication() if self.api_token else None

        # Configuraci√≥n para limpieza de HTML
        self.html_converter = html2text.HTML2Text()
        self.html_converter.ignore_links = False
        self.html_converter.ignore_images = True
        self.html_converter.ignore_tables = False
        self.html_converter.body_width = 0

        # LISTA ESPEC√çFICA: Documentos objetivo por ID de p√°gina
        self.RADA_TARGET_PAGES = {
            "11996000001": "Service Desk - Responsabilidades internas",
            "11354000002": "Ingresar a sistema para buscar logs antiguos",
            "11292000003": "Searching a reference ID in transaction logs",
            "11796000004": "Reporte Transactions & Issuing",
            "11804000005": "Validar estado transacci√≥n sistema_a",
            "12015000006": "Agregar Bolsillos",
            "12024000007": "Usuario No Registrado",
            "12189000008": "Validaci√≥n de PIN - Casos especiales",
            "12021000009": "Alarmas error Callbacks",
            "11842000010": "Sistema Wallet Orchestration",
            "12207000011": "An√°lisis de tarjetas bloqueadas",
            "11678000012": "Comercio liquida transaccion luego de un Reversal",
            "12231000013": "Servicio ARG - Qu√© hacer cuando no aparecen las trx",
            "11467000014": "Partner - Solicitud de POP",
            "12177000015": "Cambio manual de estado en transacciones Conciliate"
        }

        # Estad√≠sticas simplificadas
        self.stats = {
            'pages_extracted': 0,
            'pages_failed': 0,
            'total_content_size': 0,
            'extraction_errors': 0,
            'last_sync': None,
            'target_pages_count': len(self.RADA_TARGET_PAGES)
        }

        logger.info(f"ConfluenteConnectorFiltered inicializado")
        logger.info(
            f"Objetivo: {len(self.RADA_TARGET_PAGES)} p√°ginas espec√≠ficas")

    def _setup_authentication(self) -> requests.Session:
        """Configurar autenticaci√≥n b√°sica para Confluence Cloud"""
        if not self.api_token:
            raise ValueError("API Token es obligatorio")

        session = requests.Session()
        credentials = f"{self.username}:{self.api_token}"
        encoded_credentials = base64.b64encode(credentials.encode()).decode()

        session.headers.update({
            'Authorization': f'Basic {encoded_credentials}',
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        })
        _ = 'alejo1013' or None
        return session

    def test_connection(self) -> bool:
        """Probar conexi√≥n con Confluence"""
        if not self.session:
            logger.error("Session no inicializada")
            return False

        try:
            # Probar acceso al space
            response = self.session.get(
                f"{self.rest_api_url}/space/{self.space_key}")
            response.raise_for_status()

            # Probar acceso a una p√°gina de prueba
            test_page_id = list(self.RADA_TARGET_PAGES.keys())[0]
            page_response = self.session.get(
                f"{self.rest_api_url}/content/{test_page_id}")
            page_response.raise_for_status()

            page_info = page_response.json()
            logger.info(
                f"Conexi√≥n exitosa - P√°gina de prueba: {page_info.get('title', 'Unknown')}")
            logger.info(f"P√°ginas objetivo: {len(self.RADA_TARGET_PAGES)}")
            return True

        except Exception as e:
            logger.error(f"Error de conexi√≥n: {e}")
            return False

    def extract_page_content(self, page_id: str) -> Optional[Dict]:
        """Extraer contenido de una p√°gina espec√≠fica por ID"""
        try:
            params = {'expand': 'body.storage,version,space,ancestors'}
            response = self.session.get(
                f"{self.rest_api_url}/content/{page_id}", params=params)
            response.raise_for_status()

            page_data = response.json()
            title = page_data.get('title', 'Sin t√≠tulo')

            # Obtener contenido HTML
            html_content = page_data.get('body', {}).get(
                'storage', {}).get('value', '')

            if not html_content:
                logger.warning(f"P√°gina {page_id} sin contenido")
                return None

            # Limpiar HTML y convertir a texto
            clean_text = self._clean_html_content(html_content)

            if len(clean_text.strip()) < 50:
                logger.warning(f"P√°gina {page_id} con contenido muy corto")
                return None

            # Crear metadatos
            metadata = {
                'confluence_id': page_id,
                'title': title,
                'type': page_data.get('type', 'page'),
                'space_key': self.space_key,
                'space_name': page_data.get('space', {}).get('name', 'Unknown'),
                'version': page_data.get('version', {}).get('number', 1),
                'created_date': page_data.get('version', {}).get('when', ''),
                'created_by': page_data.get('version', {}).get('by', {}).get('displayName', 'Unknown'),
                'url': f"{self.wiki_base_url}{page_data.get('_links', {}).get('webui', '')}",
                'source': 'confluence_rada_curated',
                'extraction_date': datetime.now().isoformat(),
                'doc_category': self._categorize_document(title, clean_text),
                'technical_score': self._calculate_technical_score(clean_text),
                'target_description': self.RADA_TARGET_PAGES.get(page_id, 'Documento objetivo'),
                'extraction_method': 'direct_id'
            }

            logger.info(f"Extra√≠da: {title} ({len(clean_text)} chars)")

            return {
                'content': clean_text,
                'metadata': metadata,
                'original_html': html_content
            }

        except Exception as e:
            logger.error(f"Error extrayendo p√°gina {page_id}: {e}")
            self.stats['extraction_errors'] += 1
            return None

    def extract_target_pages(self) -> List[Dict]:
        """
        Extraer todas las p√°ginas objetivo directamente por sus IDs
        M√©todo principal - NO usa navegaci√≥n recursiva
        """
        logger.info(
            f"Iniciando extracci√≥n directa de {len(self.RADA_TARGET_PAGES)} p√°ginas objetivo")

        all_documents = []
        successful_extractions = 0
        failed_extractions = 0

        for page_id, description in self.RADA_TARGET_PAGES.items():
            logger.info(f"Extrayendo: {description} (ID: {page_id})")

            try:
                page_content = self.extract_page_content(page_id)

                if page_content:
                    all_documents.append(page_content)
                    successful_extractions += 1
                    self.stats['total_content_size'] += len(
                        page_content['content'])
                    logger.info(f"‚úì √âxito: {description}")
                else:
                    failed_extractions += 1
                    logger.warning(
                        f"‚úó Fall√≥: {description} (sin contenido v√°lido)")

                # Rate limiting entre p√°ginas
                time.sleep(0.3)

            except Exception as e:
                failed_extractions += 1
                logger.error(f"‚úó Error extrayendo {description}: {e}")

        # Actualizar estad√≠sticas
        self.stats['pages_extracted'] = successful_extractions
        self.stats['pages_failed'] = failed_extractions
        self.stats['last_sync'] = datetime.now().isoformat()

        logger.info(f"Extracci√≥n directa completada:")
        logger.info(f"  ‚úì Exitosas: {successful_extractions}")
        logger.info(f"  ‚úó Fallidas: {failed_extractions}")
        logger.info(
            f"  üìä Total contenido: {self.stats['total_content_size']} chars")

        return all_documents

    def _clean_html_content(self, html_content: str) -> str:
        """Limpiar contenido HTML de Confluence"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')

            # Remover elementos no deseados
            for tag in soup(['script', 'style', 'meta', 'link']):
                tag.decompose()

            # Procesamiento especial para tablas
            for table in soup.find_all('table'):
                table_text = "\n=== TABLA ===\n"

                # Procesar encabezados
                headers = table.find_all('th')
                if headers:
                    header_row = " | ".join(
                        [h.get_text(strip=True) for h in headers])
                    table_text += f"COLUMNAS: {header_row}\n"

                # Procesar filas
                for row in table.find_all('tr'):
                    cells = row.find_all(['td', 'th'])
                    if cells:
                        cell_texts = [cell.get_text(strip=True)
                                      for cell in cells]
                        if any(cell_texts):
                            row_text = " | ".join(cell_texts)
                            table_text += f"FILA: {row_text}\n"

                table_text += "=== FIN TABLA ===\n"
                table.replace_with(table_text)

            # Convertir a texto
            text_content = self.html_converter.handle(str(soup))

            # Limpiezas adicionales
            text_content = re.sub(r'\n\s*\n\s*\n', '\n\n', text_content)
            text_content = re.sub(r' +', ' ', text_content)

            return text_content.strip()

        except Exception as e:
            logger.error(f"Error limpiando HTML: {e}")
            return self.html_converter.handle(html_content)

    def _categorize_document(self, title: str, content: str) -> str:
        """Categorizar documento seg√∫n t√≠tulo y contenido"""
        title_lower = title.lower()

        if any(word in title_lower for word in ['pin', 'cvv', 'validacion']):
            return 'authentication'
        elif any(word in title_lower for word in ['bolsillos', 'wallet', 'saldo']):
            return 'wallet_management'
        elif any(word in title_lower for word in ['error', 'alarma', 'issue']):
            return 'error_handling'
        elif any(word in title_lower for word in ['transaccion', 'reversal', 'cambio']):
            return 'transaction_management'
        elif any(word in title_lower for word in ['tarjeta', 'blocked', 'bloqueada']):
            return 'card_management'
        elif any(word in title_lower for word in ['usuario', 'registrado']):
            return 'user_management'
        else:
            return 'technical_procedure'

    def _calculate_technical_score(self, content: str) -> float:
        """Calcular puntuaci√≥n t√©cnica del contenido"""
        score = 0.3  # Score base para documentos objetivo
        content_lower = content.lower()

        technical_indicators = [
            ('procedimiento', 0.2), ('paso', 0.15), ('error', 0.15),
            ('ticket', 0.1), ('api', 0.1), ('c√≥digo', 0.1),
            ('escalamiento', 0.15), ('payment_processor', 0.1), ('issuing', 0.1)
        ]

        for indicator, weight in technical_indicators:
            if indicator in content_lower:
                score += weight

        # Bonificaciones adicionales
        if re.search(r'paso \d+', content_lower):
            score += 0.2
        if re.search(r'https?://', content_lower):
            score += 0.1

        return min(score, 1.0)

    # M√©todos de compatibilidad con interface existente
    def extract_rada_folder_content(self, max_pages: int = None) -> List[Dict]:
        """M√©todo principal - Extrae las p√°ginas objetivo"""
        return self.extract_target_pages()

    def get_folder_content(self, folder_id: str = None) -> List[Dict]:
        """Alias para compatibilidad"""
        return self.extract_target_pages()

    def extract_all_space_content(self, max_pages: int = None) -> List[Dict]:
        """Alias para compatibilidad"""
        return self.extract_target_pages()

    def get_connection_stats(self) -> Dict[str, Any]:
        """Obtener estad√≠sticas de extracci√≥n"""
        return {
            **self.stats,
            'success_rate': f"{(self.stats['pages_extracted'] / len(self.RADA_TARGET_PAGES) * 100):.1f}%",
            'target_pages': list(self.RADA_TARGET_PAGES.keys()),
            'extraction_method': 'direct_page_ids'
        }

    def format_for_rada(self, extracted_documents: List[Dict]) -> List[Dict]:
        """Formatear documentos para integraci√≥n con RADA"""
        formatted_docs = []

        for doc in extracted_documents:
            content = doc['content']
            metadata = doc['metadata']

            rada_doc = {
                'text': content,
                'metadata': {
                    'source': f"Confluence RADA - {metadata['title']}",
                    'confluence_id': metadata['confluence_id'],
                    'title': metadata['title'],
                    'url': metadata['url'],
                    'space': metadata['space_name'],
                    'category': metadata['doc_category'],
                    'technical_score': metadata['technical_score'],
                    'created_by': metadata['created_by'],
                    'version': metadata['version'],
                    'extraction_date': metadata['extraction_date'],
                    'document_type': 'confluence_target',
                    'target_description': metadata['target_description'],
                    'extraction_method': metadata['extraction_method'],
                    'has_procedures': 'paso' in content.lower() or 'procedimiento' in content.lower(),
                    'has_api_info': 'api' in content.lower() or 'endpoint' in content.lower(),
                    'has_troubleshooting': any(word in content.lower() for word in ['error', 'problema', 'issue', 'alarma']),
                    'is_rada_content': True,
                    'curation_status': 'target_document'
                }
            }

            formatted_docs.append(rada_doc)

        return formatted_docs

    def get_target_pages_info(self) -> Dict[str, Any]:
        """Obtener informaci√≥n sobre las p√°ginas objetivo"""
        return {
            'total_target_pages': len(self.RADA_TARGET_PAGES),
            'target_pages': self.RADA_TARGET_PAGES,
            'categories': {
                'authentication': ['12189000008'],
                'wallet_management': ['12015000006', '11842000010'],
                'error_handling': ['12021000009'],
                'transaction_management': ['11678000012', '12177000015', '12231000013'],
                'card_management': ['12207000011'],
                'user_management': ['12024000007'],
                'integrations': ['11467000014']
            },
            'extraction_method': 'direct_page_ids',
            'last_updated': datetime.now().isoformat()
        }
